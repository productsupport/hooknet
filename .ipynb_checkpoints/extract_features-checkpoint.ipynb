{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib as mpl\n",
    "import io\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from utils.dataGenerator import DataGenerator, DataGenerator_metaData\n",
    "from utils import utils\n",
    "from PIL import Image\n",
    "\n",
    "import pydot\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "import skimage.measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tf version: \", tf.__version__)\n",
    "print(\"keras version: \", tf.keras.__version__)\n",
    "print(\"\")\n",
    "\n",
    "# GPU availability\n",
    "tf.test.gpu_device_name()\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_patches(file, mags, jpeg=True):\n",
    "    names = ['patches_20x', 'patches_10x', 'patches_05x']\n",
    "    with h5py.File(file, 'r') as f:\n",
    "        plist = []\n",
    "        for i in mags:\n",
    "            plist.append(list(f[names[i]]))\n",
    "        coords = list(f['coordinates'])\n",
    "        mask = list(f['mask'])\n",
    "\n",
    "    patches = [[] for i in range(len(mags))]\n",
    "    for pi in range(len(mags)):\n",
    "        for p in plist[pi]:\n",
    "            if jpeg:\n",
    "                patches[pi].append(jpeg2patch(p))\n",
    "            else:\n",
    "                patches[pi].append(p)\n",
    "    return patches, coords, mask\n",
    "\n",
    "def jpeg2patch(patch):\n",
    "    return Image.open(io.BytesIO(np.array(patch)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folders\n",
    "patches_folder = r\"D:\\example_patches\"\n",
    "patch_ids = [file for file in os.listdir(patches_folder) if file.endswith('.h5')]\n",
    "print(\"number of slide patches: \", len(patch_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "modeldir = r\"D:\\annotated_slides\\models\\hooknet\"\n",
    "modelfile = 'model_hooknet_exp_1.h5'\n",
    "hooknet = load_model(os.path.join(modeldir, modelfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hooknet.summary()\n",
    "keras.utils.plot_model(hooknet, os.path.join(modeldir, \"context_target_unet.png\"), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=hooknet.input,\n",
    "                      outputs=hooknet.get_layer(\"activation_31\").output)\n",
    "del hooknet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsz = 256\n",
    "mags=0, 1, 2\n",
    "i = 0\n",
    "for patch_name in patch_ids[0:1]:\n",
    "    print(\"patch name: \", patch_name)\n",
    "    patches, coords, mask = load_patches(os.path.join(patches_folder, patch_name), mags)\n",
    "    print(\"number of patches: \", len(patches))\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 6), sharex=True, sharey=True)\n",
    "    ax = axes.ravel()\n",
    "\n",
    "    ax[0].imshow(patches[0][i])\n",
    "    ax[0].set_title(\"patch @ 20x\")\n",
    "\n",
    "    ax[1].imshow(patches[1][i])\n",
    "    ax[1].set_title(\"patch @ 10x\")\n",
    "\n",
    "    ax[2].imshow(patches[2][i])\n",
    "    ax[2].set_title(\"patch @ 5x\")  \n",
    "\n",
    "    for a in ax.ravel():\n",
    "        a.axis('off')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    patch_t = np.asarray(patches[0][i])\n",
    "    patch_c = np.asarray(patches[2][i])\n",
    "   \n",
    "    patch_t = patch_t.astype('float') / 255\n",
    "    patch_c = patch_c.astype('float') / 255\n",
    "    \n",
    "    patch_c = np.reshape(patch_c, (1, tsz, tsz, -1))\n",
    "    patch_t = np.reshape(patch_t, (1, tsz, tsz, -1))\n",
    "\n",
    "            \n",
    "#     pred_c, pred_t = hooknet.predict([patch_c, patch_t])\n",
    "#     pred_c = np.reshape(pred_c, (1, tsz, tsz, -1))\n",
    "#     pred_c = np.argmax(pred_c, axis=-1)\n",
    "#     pred_c = np.reshape(pred_c, (tsz, tsz))\n",
    "#     print(\"predicted segmentation stats (context):\", np.unique(pred_c, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = encoder_model.predict([patch_c, patch_t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output_pool = skimage.measure.block_reduce(encoder_output, (1, 4, 4, 1), np.max)\n",
    "print(encoder_output_pool.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1024/128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2 \n",
    "dim=(256, 256, 3)\n",
    "mags=0, 2\n",
    "i = 0\n",
    "for patch_name in patch_ids[0:1]:\n",
    "    print(\"patch name: \", patch_name)\n",
    "    patches, coords, mask = load_patches(os.path.join(patches_folder, patch_name), mags)\n",
    "    print(\"number of mag patches: \", len(patches))\n",
    "    \n",
    "    # Number of batches to iterate over\n",
    "    steps = int(np.ceil(len(patches[0]) / batch_size))\n",
    "    print('steps: ', steps)\n",
    "    \n",
    "    features = []\n",
    "    patch_t = np.asarray(patches[0][2])\n",
    "    patch_c = np.asarray(patches[1][2])\n",
    "    print(patch_t.shape)\n",
    "    print(patch_c.shape)\n",
    "    \n",
    "    print(\"number of patches: \", len(patches[0]))\n",
    "    \n",
    "    for index in range(steps):\n",
    "        X_c = np.empty((batch_size, *dim))\n",
    "        X_t = np.empty((batch_size, *dim))\n",
    "\n",
    "        batchlist = []\n",
    "        for i in range(len(patches)):\n",
    "            batchlist.append(patches[i][index * batch_size:(index + 1) * batch_size])\n",
    "        print(\"len(batchlist): \", len(batchlist))\n",
    "#         print(batchlist)\n",
    "        \n",
    "        # Transform image in a batch to tensors\n",
    "        for i in range(batch_size):\n",
    "            patch_t = batchlist[0][i]\n",
    "            patch_c = batchlist[1][i]\n",
    "#             print(type(patch_t))\n",
    "#             print(type(patch_c))\n",
    "\n",
    "            patch_t = np.array(patch_t)\n",
    "            patch_c= np.array(patch_c)\n",
    "            patch_t = patch_t.astype('float') / 255\n",
    "            patch_c = patch_c.astype('float') / 255\n",
    "\n",
    "            X_c[i,] = patch_c\n",
    "            X_t[i,] = patch_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_folder = r\"E:\\WST_Share\\Features\"\n",
    "hooknet_features_folder = r\"D:\\1500_cases\\Features_hooknet\\Features\"\n",
    "feature_files = [file for file in os.listdir(hooknet_features_folder) if file.endswith(\".pt\")]\n",
    "print(\"num feature files: \", len(feature_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in feature_files[0:1]:\n",
    "    hn_features = torch.load(os.path.join(hooknet_features_folder, file))\n",
    "    features = torch.load(os.path.join(features_folder, file.split(\".\")[0] + \"_0\" + \".pt\"))\n",
    "\n",
    "    features_arr = features.numpy()\n",
    "    hn_features_arr = hn_features.numpy()\n",
    "    \n",
    "    print(\"hn features, min and max: \", np.min(hn_features_arr), np.max(hn_features_arr))\n",
    "    print(\"IN features :\", np.min(features_arr), np.max(features_arr))\n",
    "    hn_max_value = 1 # np.max(hn_features_arr)\n",
    "    \n",
    "    hn_features_arr_norm = np.linalg.norm(hn_features_arr, axis=1)\n",
    "    features_arr_norm = np.linalg.norm(features_arr, axis=1)\n",
    "    i = 2\n",
    "    plt.hist(hn_features_arr[i]/hn_max_value)\n",
    "#     plt.hist(features_arr[i])\n",
    "#     diff = features_arr - hn_features_arr\n",
    "#     dist = np.linalg.norm((features_arr - hn_features_arr), axis=1)\n",
    "#     print(np.min(dist), np.max(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hn_features_arr_norm.shape)\n",
    "print(hn_features_arr_norm[0:10])\n",
    "print(np.min(hn_features_arr_norm), np.max(hn_features_arr_norm))\n",
    "print(features_arr_norm[0:10])\n",
    "print(np.min(features_arr_norm), np.max(features_arr_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### normalize the hooknet features to [0, 1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in feature_files:\n",
    "    hn_features = torch.load(os.path.join(hooknet_features_folder, file))\n",
    "\n",
    "    hn_features_arr = hn_features.numpy()\n",
    "    hn_max_value = np.max(hn_features_arr)\n",
    "#     print(\"max value is: \", hn_max_value)\n",
    "    \n",
    "    hn_features_arr = hn_features_arr/hn_max_value\n",
    "#     print(hn_max_value.shape)\n",
    "#     print(np.max(hn_features_arr))\n",
    "\n",
    "    \n",
    "    features = torch.from_numpy(hn_features_arr)\n",
    "    torch.save(features, os.path.join(\n",
    "        \"D:\\\\1500_cases\\\\Norm_Features_hooknet\\\\Features\", file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create cvs file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"D:\\\\20kDatasetMetafiles\\\\patientinfo_train_smallDataset_1500.csv\")\n",
    "hooknet_features_folder = r\"D:\\1500_cases\\Features_hooknet\\Features\"\n",
    "feature_files = [file for file in os.listdir(hooknet_features_folder) if file.endswith(\".pt\")]\n",
    "print(\"num feature files: \", len(feature_files))\n",
    "\n",
    "df['new_slide_id'] = df['slide_id'].apply(lambda x: x.rsplit(\"_\", 1)[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [name.split(\".\")[0] for name in feature_files]\n",
    "print(file_names[0:4])\n",
    "\n",
    "df = df[df['new_slide_id'].isin(file_names)]\n",
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['slide_id'], axis=1)\n",
    "df = df.rename(columns={\"new_slide_id\": \"slide_id\"})\n",
    "df.to_csv(r\"D:\\1500_cases\\Features_hooknet\\df_train.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "df = pd.read_csv(r\"D:\\\\20kDatasetMetafiles\\\\patientinfo_test_smallDataset.csv\")\n",
    "hooknet_features_folder = r\"D:\\1500_cases\\Features_hooknet\\Features\"\n",
    "feature_files = [file for file in os.listdir(hooknet_features_folder) if file.endswith(\".pt\")]\n",
    "print(\"num feature files: \", len(feature_files))\n",
    "\n",
    "df['new_slide_id'] = df['slide_id'].apply(lambda x: x.rsplit(\"_\", 1)[0])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [name.split(\".\")[0] for name in feature_files]\n",
    "print(file_names[0:4])\n",
    "\n",
    "df = df[df['new_slide_id'].isin(file_names)]\n",
    "print(df.head())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['slide_id'], axis=1)\n",
    "df = df.rename(columns={\"new_slide_id\": \"slide_id\"})\n",
    "df.to_csv(r\"D:\\1500_cases\\Features_hooknet\\df_test.csv\", index=False)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
