{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import h5py\n",
    "import glob\n",
    "\n",
    "from utils.dataGenerator import DataGenerator, DataGenerator_metaData\n",
    "from models import models\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folders\n",
    "patches_folder = r\"D:\\annotated_slides\\separate_patches_and_labels_hooknet_exp_1\"\n",
    "svsfolder = r\"D:\\annotated_slides\\Slides\"\n",
    "\n",
    "slide_patches = glob.glob(os.path.join(patches_folder,'*.h5'))\n",
    "print(\"number of slide patches: \", len(slide_patches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train, validation and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load csv file\n",
    "# df = pd.read_csv(os.path.join(svsfolder, \"csv_file.csv\"))\n",
    "\n",
    "# slide_ids = list(df['slide_id'].values)\n",
    "# train_ids, val_test_ids = train_test_split(slide_ids, test_size=0.30, random_state=42)\n",
    "# validation_ids, test_ids = train_test_split(val_test_ids, test_size=0.35, random_state=42)\n",
    "\n",
    "# print(\"train: {:d}, validation: {:d}, test: {:d}\".format(len(train_ids), len(validation_ids), len(test_ids)))  \n",
    "\n",
    "# train_patches = []\n",
    "# validation_patches = []\n",
    "# test_patches = []\n",
    "# for patch_path in slide_patches:\n",
    "#     patch_name = patch_path.split(\"\\\\\")[-1]\n",
    "    \n",
    "#     for tran_id in train_ids:\n",
    "#         if tran_id in patch_name:\n",
    "#             train_patches.append(patch_name)\n",
    "#             break\n",
    "    \n",
    "#     for val_id in validation_ids:\n",
    "#         if val_id in patch_name:\n",
    "#             validation_patches.append(patch_name)\n",
    "#             break\n",
    "            \n",
    "#     for test_id in test_ids:\n",
    "#         if test_id in patch_name:\n",
    "#             test_patches.append(patch_name)\n",
    "#             break\n",
    "            \n",
    "# # # print([x for x in train_patches if x in validation_pathes])\n",
    "# print(\"train patches: {:d}, validation patches: {:d}, test patches: {:d}\".format(\n",
    "#     len(train_patches), len(validation_patches), len(test_patches)))  \n",
    "\n",
    "# partition = {'train': train_patches,\n",
    "#              'validation': validation_patches}\n",
    "\n",
    "classmeta_train = pd.read_csv(os.path.join(patches_folder, \"classmeta_train.csv\"))\n",
    "classmeta_validation = pd.read_csv(os.path.join(patches_folder, \"classmeta_validation.csv\"))\n",
    "classmeta_test = pd.read_csv(os.path.join(patches_folder, \"classmeta_test.csv\"))\n",
    "\n",
    "print(\"train patches: {:d}, validation patches: {:d}, test patches: {:d}\".format(\n",
    "    len(classmeta_train), len(classmeta_validation), len(classmeta_test)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # calculate sample weights\n",
    "# class_counts = {}\n",
    "# for patch in train_patches[0:20]:\n",
    "#     patchfile = os.path.join(patches_folder, patch)  # train_patches[1])\n",
    "# #     print(\"patchfile: \", patchfile)\n",
    "    \n",
    "#     with h5py.File(patchfile, 'r') as f:\n",
    "#         seg = f['patches_20x']['segmentation'][:]\n",
    "#         labels, counts = np.unique(seg, return_counts=True)\n",
    "#         dict_count = dict(zip(labels, counts))\n",
    "# #         print(dict_count)\n",
    "        \n",
    "#         for label in labels:\n",
    "#             print(label)\n",
    "#             if label in class_counts.keys():\n",
    "#                 class_counts[label] += dict_count[label]\n",
    "#             else:\n",
    "#                 class_counts[label] = dict_count[label]\n",
    "                \n",
    "# print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(patchfile, 'r') as f:\n",
    "#     print(list(f.keys()))\n",
    "#     print(list(f.values()))\n",
    "\n",
    "#     patch = f['patches_20x']['patch'][:]\n",
    "#     seg = f['patches_20x']['segmentation'][:]\n",
    "#     print(patch.shape)\n",
    "#     print(patch.dtype)\n",
    "#     print(seg.shape)\n",
    "#     print(seg.dtype)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = r\"D:\\annotated_slides\"\n",
    "modeldir = os.path.join(main_folder, 'models')\n",
    "os.makedirs(modeldir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim': (256, 256),\n",
    "          'batch_size': 14, \n",
    "          'n_classes': 7,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True,\n",
    "          'data_folder': r\"D:\\annotated_slides\\separate_patches_and_labels_hooknet_exp_1\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(modeldir, \"logs\")\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', min_delta=0, patience=50, verbose=0, mode='auto'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "#         filepath=os.path.join(modeldir, \"model.{epoch:02d}-{val_loss:.2f}.h5\"),\n",
    "        filepath=os.path.join(modeldir, \"model_hooknet_exp_1.h5\"),\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=logdir),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model architecture\n",
    "model = models.context_target_unet(context_input=params['dim'] + (params['n_channels'], ) ,\n",
    "                                   target_input= params['dim'] + (params['n_channels'], ) ,\n",
    "                                   nClass=params['n_classes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model\n",
    "# keras.utils.plot_model(model, \"context_target_unet.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4), \n",
    "#               loss='categorical_crossentropy',\n",
    "# #               sample_weight_mode=\"temporal\",\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss={\n",
    "        \"out_context\": tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        \"out_target\": keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    },\n",
    "    sample_weight_mode={\n",
    "        \"out_context\": \"temporal\",\n",
    "        \"out_target\": \"temporal\"\n",
    "    },\n",
    "    loss_weights=[0.25, 0.75],\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator_metaData(classmeta_train, **params)\n",
    "validation_generator = DataGenerator_metaData(classmeta_validation, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example data generator\n",
    "max_iter = 1  # maximum number of iterations, in each iteration one batch is generated; the proper value depends on batch size and size of whole data\n",
    "i = 0\n",
    "for (dc, dt), (lc, lt), (swc, swt) in training_generator:\n",
    "    i += 1\n",
    "    if i == max_iter:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(dc[i])\n",
    "ax[1].imshow(dt[i])\n",
    "\n",
    "print(\"swc: \", np.unique(swc[i], return_counts=True))\n",
    "print(\"swt: \", np.unique(swt[i], return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    callbacks=my_callbacks,\n",
    "                    epochs=200)\n",
    "#                     use_multiprocessing=True,\n",
    "#                     workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add data augmenttaion\n",
    "# add option for different magnifications\n",
    "# class/sample weights: calculate over a batch or entire training data (what are the class stats??) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 11\n",
    "mask = np.random.randint(0, n_classes-8, (256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(mask, return_counts=True)\n",
    "dict_count = dict(zip(unique, counts))\n",
    "print(\"dict_count_1: \", dict_count)\n",
    "\n",
    "dict_count.pop(0, None)\n",
    "print(\"dict_count_2: \", dict_count)\n",
    "\n",
    "# if a class is not there, add count 0 for that class ???\n",
    "for i in range(1, n_classes):\n",
    "    if i not in dict_count.keys():\n",
    "        dict_count[i] = 0\n",
    "        \n",
    "print(\"dict_count_3: \", dict_count)\n",
    "\n",
    "counts_sum = 0\n",
    "for i in dict_count.keys():\n",
    "    counts_sum += dict_count[i]\n",
    "    \n",
    "print(\"counts sum:\", counts_sum)\n",
    "\n",
    "# calculate weights\n",
    "class_weight_patch_dict = {}\n",
    "for key in dict_count.keys():\n",
    "    class_weight_patch_dict[key] = 1.0 - dict_count[key] / counts_sum\n",
    "\n",
    "print(\"class_weight_patch_dict: \", class_weight_patch_dict)\n",
    "\n",
    "weights_sum = 0\n",
    "for i in class_weight_patch_dict.keys():\n",
    "    weights_sum += class_weight_patch_dict[i]\n",
    "\n",
    "print(\"weights_sum: \", weights_sum)\n",
    "\n",
    "class_weight_patch_norm_dict = {}\n",
    "for key in class_weight_patch_dict.keys():\n",
    "    class_weight_patch_norm_dict[key] = class_weight_patch_dict[key] / weights_sum\n",
    "\n",
    "print(\"class_weight_patch_norm_dict\", class_weight_patch_norm_dict)\n",
    "\n",
    "weights_sum = 0\n",
    "for i in class_weight_patch_norm_dict:\n",
    "    weights_sum += class_weight_patch_norm_dict[i]\n",
    "\n",
    "print(\"weights_sum norm: \", weights_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
