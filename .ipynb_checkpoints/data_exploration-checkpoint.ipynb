{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib as mpl\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from utils.dataGenerator import DataGenerator\n",
    "from utils import utils\n",
    "\n",
    "import xml.etree.ElementTree as et\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of slide:  72\n",
      "number of annotated slides:  72\n"
     ]
    }
   ],
   "source": [
    "svsfolder = r\"D:\\annotated_slides\\Slides\"\n",
    "annotation_folder = r\"D:\\annotated_slides\\Annotations\"\n",
    "\n",
    "svsfiles = glob.glob(os.path.join(svsfolder,'*.svs'))\n",
    "print(\"number of slide: \", len(svsfiles))\n",
    "\n",
    "xmlfiles = glob.glob(os.path.join(annotation_folder,'*.xml'))\n",
    "print(\"number of annotated slides: \", len(xmlfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_class_labels:  ['Benigne', 'Muscle', 'Lymfocyten', 'MC', 'ILC', 'None', 'Stroma', 'Mastopatic', 'NormalEpithelial', 'Adipose', 'RedBlood', 'Reactive changes', 'Necrosis', 'Micro calcification', 'IDC', 'CIS']\n"
     ]
    }
   ],
   "source": [
    "# find all the class labels used for annotations\n",
    "all_class_labels = [] \n",
    "for xmlfile in xmlfiles:\n",
    "    anno_dict = {}\n",
    "    xmlbase = os.path.splitext(os.path.basename(xmlfile))[0]\n",
    "    tree = et.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    annotations = root.iter('Annotation')\n",
    "    label = []\n",
    "    for elem in annotations:\n",
    "        # Loop over annotations\n",
    "        label.append(elem.get('PartOfGroup'))\n",
    "    all_class_labels.append(label) \n",
    "\n",
    "all_class_labels = [x for xlist in all_class_labels for x in xlist]\n",
    "unique_class_labels = list(set(all_class_labels))\n",
    "print(\"unique_class_labels: \", unique_class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = unique_class_labels.insert(0, 'slide_id')\n",
    "df_annotations = pd.DataFrame(columns=column_names)\n",
    "\n",
    "for xmlfile in xmlfiles:\n",
    "    anno_dict = {}\n",
    "    xmlbase = os.path.splitext(os.path.basename(xmlfile))[0]\n",
    "    tree = et.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    annotations = root.iter('Annotation')\n",
    "    label = []\n",
    "    for elem in annotations:\n",
    "        # Loop over annotations\n",
    "        label.append(elem.get('PartOfGroup'))\n",
    "    \n",
    "    count_dict = dict(Counter(label).items())\n",
    "    \n",
    "    for name in unique_class_labels:\n",
    "        if name in count_dict.keys():\n",
    "            anno_dict[name] = count_dict[name]\n",
    "        else:\n",
    "            anno_dict[name] = 0\n",
    "            \n",
    "    anno_dict['slide_id'] = xmlbase\n",
    "    \n",
    "    df_annotations = df_annotations.append(anno_dict, ignore_index=True)\n",
    "\n",
    "# move 'slide_id column to front'\n",
    "cols = list(df_annotations)\n",
    "cols.insert(0, cols.pop(cols.index('slide_id')))\n",
    "df_annotations = df_annotations.loc[:, cols]\n",
    "df_annotations.to_csv(os.path.join(\"D:\\\\annotated_slides\", \"df_annotations_meta.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### new classes\n",
    "classnames = ['InvTumour', 'Stroma', 'Adipose', 'Other']\n",
    "InvTumourClass = ['CIS', 'IDC', 'ILC', 'MC', 'Necrosis']\n",
    "OtherClass = ['RedBlood', 'Benigne', 'NormalEpithelial', 'Mastopatic', 'Micro calcification', 'Lymfocyten', 'Muscle', 'Reactive changes']\n",
    "\n",
    "column_names = classnames.insert(0, 'slide_id')\n",
    "df_annotations = pd.DataFrame(columns=column_names)\n",
    "\n",
    "for xmlfile in xmlfiles:\n",
    "    anno_dict = {}\n",
    "    xmlbase = os.path.splitext(os.path.basename(xmlfile))[0]\n",
    "    tree = et.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    annotations = root.iter('Annotation')\n",
    "    label = []\n",
    "    for elem in annotations:\n",
    "        # Loop over annotations\n",
    "        label_temp = elem.get('PartOfGroup')\n",
    "        if label_temp in OtherClass:\n",
    "            label_temp = 'Other'\n",
    "            label.append(label_temp)\n",
    "        elif label_temp in InvTumourClass:\n",
    "            label_temp = 'InvTumour'\n",
    "            label.append(label_temp)\n",
    "        else:\n",
    "            label.append(label_temp)\n",
    "    \n",
    "    count_dict = dict(Counter(label).items())\n",
    "    \n",
    "    for name in classnames:\n",
    "        if name in count_dict.keys():\n",
    "            anno_dict[name] = count_dict[name]\n",
    "        else:\n",
    "            anno_dict[name] = 0\n",
    "            \n",
    "    anno_dict['slide_id'] = xmlbase\n",
    "    \n",
    "    df_annotations = df_annotations.append(anno_dict, ignore_index=True)\n",
    "\n",
    "# move 'slide_id column to front'\n",
    "cols = list(df_annotations)\n",
    "cols.insert(0, cols.pop(cols.index('slide_id')))\n",
    "df_annotations = df_annotations.loc[:, cols]\n",
    "df_annotations.to_csv(os.path.join(\"D:\\\\annotated_slides\", \"df_annotations_meta_4classes.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slide_id</th>\n",
       "      <th>Adipose</th>\n",
       "      <th>InvTumour</th>\n",
       "      <th>Other</th>\n",
       "      <th>Stroma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HE_FFP15-004590I1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HE_FFP15-006979I1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HE_FFP15-007865I1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HE_FFP15-008475I1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HE_FFP15-008820I2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>HE_FFP19-011460I1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>HE_FFP19-015559I1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>HE_FFP19-016787I1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>HE_FFP19-018435I1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>HE_FFP19-018490I1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             slide_id  Adipose  InvTumour  Other  Stroma\n",
       "0   HE_FFP15-004590I1      3.0        4.0    5.0     1.0\n",
       "1   HE_FFP15-006979I1      0.0        1.0   20.0     0.0\n",
       "2   HE_FFP15-007865I1      2.0        4.0    4.0     1.0\n",
       "3   HE_FFP15-008475I1      2.0        4.0    4.0     2.0\n",
       "4   HE_FFP15-008820I2      0.0        3.0    5.0     1.0\n",
       "..                ...      ...        ...    ...     ...\n",
       "67  HE_FFP19-011460I1      1.0        3.0    3.0     0.0\n",
       "68  HE_FFP19-015559I1      1.0        5.0    3.0     0.0\n",
       "69  HE_FFP19-016787I1      0.0        6.0    0.0     0.0\n",
       "70  HE_FFP19-018435I1      1.0        2.0    2.0     1.0\n",
       "71  HE_FFP19-018490I1      1.0       10.0    8.0     5.0\n",
       "\n",
       "[72 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extracted patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib as mpl\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from utils.dataGenerator import DataGenerator\n",
    "from utils import utils\n",
    "\n",
    "import xml.etree.ElementTree as et\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of slide patches:  127082\n"
     ]
    }
   ],
   "source": [
    "# data folders\n",
    "patches_folder = r\"D:\\annotated_slides\\separate_patches_and_labels\"\n",
    "svsfolder = r\"D:\\annotated_slides\\Slides\"\n",
    "\n",
    "slide_patches = glob.glob(os.path.join(patches_folder,'*.h5'))\n",
    "print(\"number of slide patches: \", len(slide_patches))\n",
    "\n",
    "annotation_folder = r\"D:\\annotated_slides\\Annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 50 slides, validation: 22 slides\n",
      "train patches: 68526, validation patches: 58556\n"
     ]
    }
   ],
   "source": [
    "# Load csv file\n",
    "df = pd.read_csv(os.path.join(svsfolder, \"csv_file.csv\"))\n",
    "\n",
    "slide_ids = list(df['slide_id'].values)\n",
    "train_ids, validation_ids = train_test_split(slide_ids, test_size=0.30, random_state=42)\n",
    "\n",
    "print(\"train: {:d} slides, validation: {:d} slides\".format(\n",
    "    len(train_ids), len(validation_ids)))  \n",
    "\n",
    "train_patches = []\n",
    "validation_patches = []\n",
    "for patch_path in slide_patches:\n",
    "    patch_name = patch_path.split(\"\\\\\")[-1]\n",
    "    \n",
    "    for tran_id in train_ids:\n",
    "        if tran_id in patch_name:\n",
    "            train_patches.append(patch_name)\n",
    "            break\n",
    "    \n",
    "    for val_id in validation_ids:\n",
    "        if val_id in patch_name:\n",
    "            validation_patches.append(patch_name)\n",
    "            break\n",
    "            \n",
    "# # print([x for x in train_patches if x in validation_pathes])\n",
    "print(\"train patches: {:d}, validation patches: {:d}\".format(\n",
    "    len(train_patches), len(validation_patches)))  \n",
    "\n",
    "partition = {'train': train_patches,\n",
    "             'validation': validation_patches\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  ['Adipose', 'InvTumour', 'Other', 'Stroma']\n",
      "new class names:  ['unknown', 'Adipose', 'InvTumour', 'Other', 'Stroma']\n"
     ]
    }
   ],
   "source": [
    "classnames = ['InvTumour', 'Stroma', 'Adipose', 'Other']\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "if classnames is not None:\n",
    "    le.fit(classnames)\n",
    "    print('classes: ', list(le.classes_))\n",
    "    new_class_names = ['unknown'] + list(le.classes_)\n",
    "    \n",
    "print(\"new class names: \", new_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate stats per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_counts(patches):\n",
    "    class_counts = {}\n",
    "    for patch_name in patches:\n",
    "        with h5py.File(os.path.join(patches_folder, patch_name), 'r') as f:\n",
    "            seg = f['patches_20x']['segmentation'][:]\n",
    "#             patch = f['patches_20x']['patch'][:]\n",
    "            seg_ids, seg_counts = np.unique(seg, return_counts=True)\n",
    "#            print(seg_ids, seg_counts)\n",
    "            for i in range(len(seg_ids)):\n",
    "                seg_label = new_class_names[seg_ids[i]]\n",
    "                seg_count = seg_counts[i]\n",
    "            \n",
    "                if seg_label in class_counts.keys():\n",
    "                    class_counts[seg_label] += seg_count\n",
    "                else:\n",
    "                    class_counts[seg_label] = seg_count\n",
    "    return class_counts\n",
    "\n",
    "\n",
    "def returnPerc(myDict, name):\n",
    "    myDict.pop(\"unknown\", None)\n",
    "    perc = {}\n",
    "    perc['dataset'] = name\n",
    "    sum = 0\n",
    "    for i in myDict: \n",
    "        sum += myDict[i] \n",
    "    \n",
    "    for key, value in myDict.items():\n",
    "        perc[key] = value/sum\n",
    "    \n",
    "    return perc\n",
    "\n",
    "\n",
    "def metaData(patches):\n",
    "    classmeta = pd.DataFrame()\n",
    "    i = 0 \n",
    "    for si, patch_name in enumerate(patches):\n",
    "        with h5py.File(os.path.join(patches_folder, patch_name), 'r') as f:\n",
    "            seg = f['patches_20x']['segmentation'][:]\n",
    "            seg_ids, seg_counts = np.unique(seg, return_counts=True)\n",
    "            \n",
    "            if len(seg_ids) <= 1:\n",
    "                metai = pd.DataFrame(data={'patch_name': patch_name,\n",
    "                                           'class_id': list(seg_ids),\n",
    "                                           'npixels': list(seg_counts)})\n",
    "                if si == 0:\n",
    "                    classmeta = metai\n",
    "                else:\n",
    "                    classmeta = pd.concat([classmeta, metai], ignore_index=True)\n",
    "            \n",
    "            else:\n",
    "                i += 1\n",
    "    print(i)\n",
    "    return classmeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n",
      "122\n"
     ]
    }
   ],
   "source": [
    "classmeta_train = metaData(train_patches)\n",
    "classmeta_validation = metaData(validation_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0\n",
      "validation:  Empty DataFrame\n",
      "Columns: [patch_name, class_id, npixels]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patch_name</th>\n",
       "      <th>class_id</th>\n",
       "      <th>npixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HE_FFP15-004590I1_0.h5</td>\n",
       "      <td>4</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HE_FFP15-004590I1_1.h5</td>\n",
       "      <td>4</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HE_FFP15-004590I1_10.h5</td>\n",
       "      <td>4</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HE_FFP15-004590I1_100.h5</td>\n",
       "      <td>1</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HE_FFP15-004590I1_1000.h5</td>\n",
       "      <td>1</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58429</th>\n",
       "      <td>HE_FFP19-006435I1_95.h5</td>\n",
       "      <td>2</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58430</th>\n",
       "      <td>HE_FFP19-006435I1_96.h5</td>\n",
       "      <td>2</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58431</th>\n",
       "      <td>HE_FFP19-006435I1_97.h5</td>\n",
       "      <td>2</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58432</th>\n",
       "      <td>HE_FFP19-006435I1_98.h5</td>\n",
       "      <td>2</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58433</th>\n",
       "      <td>HE_FFP19-006435I1_99.h5</td>\n",
       "      <td>2</td>\n",
       "      <td>65536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58434 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      patch_name  class_id  npixels\n",
       "0         HE_FFP15-004590I1_0.h5         4    65536\n",
       "1         HE_FFP15-004590I1_1.h5         4    65536\n",
       "2        HE_FFP15-004590I1_10.h5         4    65536\n",
       "3       HE_FFP15-004590I1_100.h5         1    65536\n",
       "4      HE_FFP15-004590I1_1000.h5         1    65536\n",
       "...                          ...       ...      ...\n",
       "58429    HE_FFP19-006435I1_95.h5         2    65536\n",
       "58430    HE_FFP19-006435I1_96.h5         2    65536\n",
       "58431    HE_FFP19-006435I1_97.h5         2    65536\n",
       "58432    HE_FFP19-006435I1_98.h5         2    65536\n",
       "58433    HE_FFP19-006435I1_99.h5         2    65536\n",
       "\n",
       "[58434 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of patches with class \"unknown\"\n",
    "print(\"train: \", len(classmeta_train[classmeta_train.class_id == 0]))\n",
    "print(\"validation: \", classmeta_validation[classmeta_validation.class_id == 0])\n",
    "# print(\"test: \", classmeta_test[classmeta_test.class_id == 0])\n",
    "classmeta_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      " class_id\n",
      "1    15164\n",
      "2    37882\n",
      "3     5033\n",
      "4    10280\n",
      "Name: class_id, dtype: int64\n",
      "validation: \n",
      " class_id\n",
      "1    24540\n",
      "2    28500\n",
      "3     2930\n",
      "4     2464\n",
      "Name: class_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('train: \\n', classmeta_train.groupby(['class_id'])['class_id'].count())\n",
    "print('validation: \\n', classmeta_validation.groupby(['class_id'])['class_id'].count())\n",
    "# print('test: \\n', classmeta_test.groupby(['class_id'])['class_id'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_id class_label  counts\n",
      "0         1     Adipose   15164\n",
      "1         2   InvTumour   37882\n",
      "2         3       Other    5033\n",
      "3         4      Stroma   10280\n"
     ]
    }
   ],
   "source": [
    "train_stats = classmeta_train.groupby(['class_id'])['class_id'].count()\n",
    "df_train_stats = train_stats.to_frame(name='counts')\n",
    "df_train_stats['class_label'] =  list(le.classes_)\n",
    "df_train_stats.reset_index(inplace=True)\n",
    "print(df_train_stats[['class_id', 'class_label', 'counts']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_id class_label  counts\n",
      "0         1     Adipose   24540\n",
      "1         2   InvTumour   28500\n",
      "2         3       Other    2930\n",
      "3         4      Stroma    2464\n"
     ]
    }
   ],
   "source": [
    "validation_stats = classmeta_validation.groupby(['class_id'])['class_id'].count()\n",
    "df_validation_stats = validation_stats.to_frame(name='counts')\n",
    "df_validation_stats['class_label'] =  list(le.classes_)\n",
    "df_validation_stats.reset_index(inplace=True)\n",
    "print(df_validation_stats[['class_id', 'class_label', 'counts']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_stats = classmeta_test.groupby(['class_id'])['class_id'].count()\n",
    "# df_test_stats = test_stats.to_frame(name='counts')\n",
    "# df_test_stats['class_label'] =  list(le.classes_)\n",
    "# df_test_stats.reset_index(inplace=True)\n",
    "# print(df_test_stats[['class_id', 'class_label', 'counts']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classmeta_train.to_csv(os.path.join(patches_folder, \"classmeta_train.csv\"), index=False)\n",
    "classmeta_validation.to_csv(os.path.join(patches_folder, \"classmeta_validation.csv\"), index=False)\n",
    "# classmeta_test.to_csv(os.path.join(patches_folder, \"classmeta_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classmeta_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "5033\n",
      "['HE_FFP15-006979I1_0.h5', 'HE_FFP15-006979I1_1.h5', 'HE_FFP15-006979I1_10.h5']\n",
      "2\n",
      "37882\n",
      "['HE_FFP15-006979I1_112.h5', 'HE_FFP15-006979I1_113.h5', 'HE_FFP15-006979I1_115.h5']\n",
      "1\n",
      "15164\n",
      "['HE_FFP15-007865I1_1000.h5', 'HE_FFP15-007865I1_1001.h5', 'HE_FFP15-007865I1_1002.h5']\n",
      "4\n",
      "10280\n",
      "['HE_FFP15-008475I1_105.h5', 'HE_FFP15-008475I1_120.h5', 'HE_FFP15-008475I1_121.h5']\n",
      "7500\n",
      "['HE_FFP16-007531I1_314.h5', 'HE_FFP18-017374I1_192.h5', 'HE_FFP18-006021I1_3.h5', 'HE_FFP16-004697I1_189.h5']\n"
     ]
    }
   ],
   "source": [
    "# Select one row at random for each distinct value in column class_id. use random_state for reproducibility:\n",
    "import random\n",
    "epoch_patches = []\n",
    "for class_id in classmeta_train[\"class_id\"].unique():\n",
    "    print(class_id)\n",
    "    patch_names = list(classmeta_train[classmeta_train['class_id'] == class_id]['patch_name'].values)\n",
    "    print(len(patch_names))\n",
    "    print(patch_names[0:3])\n",
    "    if class_id == 1:\n",
    "        random_patches = random.sample(patch_names, 500)\n",
    "        epoch_patches.extend(random_patches)\n",
    "    elif class_id == 2:\n",
    "        random_patches = random.sample(patch_names, 4000)\n",
    "        epoch_patches.extend(random_patches)\n",
    "    elif class_id == 3:\n",
    "        random_patches = random.sample(patch_names, 1000)\n",
    "        epoch_patches.extend(random_patches)\n",
    "    else:\n",
    "        random_patches = random.sample(patch_names, 2000)\n",
    "        epoch_patches.extend(random_patches)\n",
    "        \n",
    "print(len(epoch_patches))\n",
    "print(epoch_patches[0:4])\n",
    "# test = classmeta_train.groupby([\"class_id\"]).sample(n=2, replace=True)\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(test.patch_name.values)\n",
    "\n",
    "np.random.shuffle(indexes)\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(test[test['class_id'] == 7]['patch_name'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames = ['Mastopatic', 'CIS', 'Necrosis', 'NormalEpithelial', 'IDC', 'Stroma', 'Lymfocyten',\n",
    "              'Adipose', 'RedBlood', 'ILC']\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "if classnames is not None:\n",
    "    le.fit(classnames)\n",
    "    print('classes: ', list(le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classmeta_train.groupby(['class_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim': (256, 256),\n",
    "          'batch_size': 10, \n",
    "          'n_classes': 11,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True,\n",
    "          'data_folder': r\"D:\\annotated_slides\\separate_patches_and_labels_exp_1\"}\n",
    "\n",
    "# without augmentation\n",
    "training_generator = DataGenerator(classmeta_train, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []     # store all the generated data batches\n",
    "labels = []   # store all the generated label batches\n",
    "max_iter = 4  # maximum number of iterations, in each iteration one batch is generated; the proper value depends on batch size and size of whole data\n",
    "i = 0\n",
    "for d, l, w in training_generator:\n",
    "    data.append(d)\n",
    "    labels.append(l)\n",
    "    i += 1\n",
    "    if i == max_iter:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    \n",
    "    def __init__(self, metaData, data_folder, batch_size=10, dim=(256, 256),\n",
    "                 n_channels=3, n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.metaData = metaData\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.data_folder = data_folder\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.indexes) / self.batch_size))\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        list_IDs_temp = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "#         list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, Y, sample_weights  = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, Y, sample_weights\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        classmeta_temp = self.metaData.groupby([\"class_id\"]).sample(n=1000, replace=True)  # random_state=1\n",
    "        self.indexes = list(classmeta_temp.patch_name.values)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        Y = np.empty((self.batch_size, *self.dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # print(i, ID)\n",
    "            \n",
    "            # Store sample\n",
    "            with h5py.File(os.path.join(self.data_folder, ID), 'r') as f:\n",
    "                patch = f['patches_20x']['patch'][:]\n",
    "                seg = f['patches_20x']['segmentation'][:]\n",
    "                patch = patch.astype('float') / 255\n",
    "                # seg = seg.astype('uint')\n",
    "#                 print(np.unique(seg, return_counts=True))\n",
    "                # print(patch.shape, seg.shape)\n",
    "\n",
    "                X[i, ] = patch  # f['patches_20x']['patch'][:]\n",
    "                Y[i, ] = seg  # f['patches_20x']['segmentation'][:]\n",
    "\n",
    "\n",
    "\n",
    "#         # sample weights: approach 1: 0 for unknown class and 1 for the rest\n",
    "#         sample_weights = np.ones_like(Y)\n",
    "#         sample_weights = sample_weights.astype(dtype='float32')\n",
    "#         sample_weights[np.where(Y == 0)] = 0.0\n",
    "        \n",
    "#         print(np.unique(Y, return_counts=True))\n",
    "        Y = keras.utils.to_categorical(Y, num_classes=self.n_classes)\n",
    "\n",
    "#         sample_weights = np.reshape(\n",
    "#             sample_weights, (sample_weights.shape[0],\n",
    "#                              sample_weights.shape[1] * sample_weights.shape[2]))\n",
    "\n",
    "#         # reshape to be able to use sample_weights\n",
    "#         Y = np.reshape(Y, (Y.shape[0], Y.shape[1] * Y.shape[2], -1))\n",
    "\n",
    "        return X, Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class counts and percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts_train = calculate_class_counts(train_patches)\n",
    "class_counts_validation = calculate_class_counts(validation_patches)\n",
    "class_counts_test = calculate_class_counts(test_patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_counts_train)\n",
    "class_perc_train = returnPerc(class_counts_train, \"train\")\n",
    "print(\"class_perc_train: \", class_perc_train)\n",
    "print(\"\")\n",
    "class_perc_validation = returnPerc(class_counts_validation, \"validation\")\n",
    "print(\"class_perc_validation: \", class_perc_validation)\n",
    "print(\"\")\n",
    "class_perc_test = returnPerc(class_counts_test, \"test\")\n",
    "print(\"class_perc_test: \", class_perc_test)\n",
    "\n",
    "# df_class_perc_train = pd.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class_perc_train = pd.DataFrame.from_dict([class_perc_train])\n",
    "df_class_perc_validation = pd.DataFrame.from_dict([class_perc_validation])\n",
    "df_class_perc_test = pd.DataFrame.from_dict([class_perc_test])\n",
    "df_class_perc = pd.concat([df_class_perc_train, df_class_perc_validation, df_class_perc_test])\n",
    "df_class_perc = df_class_perc.set_index(\"dataset\")\n",
    "df_class_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
