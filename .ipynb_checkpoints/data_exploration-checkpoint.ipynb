{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import matplotlib as mpl\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from utils.dataGenerator import DataGenerator\n",
    "from utils import utils\n",
    "\n",
    "import xml.etree.ElementTree as et\n",
    "from collections import Counter\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of slide patches:  122997\n"
     ]
    }
   ],
   "source": [
    "# data folders\n",
    "patches_folder = r\"D:\\annotated_slides\\separate_patches_and_labels_hooknet_exp_1\"\n",
    "svsfolder = r\"D:\\annotated_slides\\Slides\"\n",
    "\n",
    "slide_patches = glob.glob(os.path.join(patches_folder,'*.h5'))\n",
    "print(\"number of slide patches: \", len(slide_patches))\n",
    "\n",
    "annotation_folder = r\"D:\\annotated_slides\\Annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 50 slides, validation: 19 slides, test: 3 slides\n",
      "train patches: 65146, validation patches: 55068, test patches: 2783\n"
     ]
    }
   ],
   "source": [
    "# Load csv file\n",
    "df = pd.read_csv(os.path.join(svsfolder, \"csv_file.csv\"))\n",
    "\n",
    "slide_ids = list(df['slide_id'].values)\n",
    "train_ids, val_test_ids = train_test_split(slide_ids, test_size=0.30, random_state=42)\n",
    "validation_ids, test_ids = train_test_split(val_test_ids, test_size=0.10, random_state=42)\n",
    "\n",
    "print(\"train: {:d} slides, validation: {:d} slides, test: {:d} slides\".format(\n",
    "    len(train_ids), len(validation_ids), len(test_ids)))  \n",
    "\n",
    "train_patches = []\n",
    "validation_patches = []\n",
    "test_patches = []\n",
    "for patch_path in slide_patches:\n",
    "    patch_name = patch_path.split(\"\\\\\")[-1]\n",
    "    \n",
    "    for tran_id in train_ids:\n",
    "        if tran_id in patch_name:\n",
    "            train_patches.append(patch_name)\n",
    "            break\n",
    "    \n",
    "    for val_id in validation_ids:\n",
    "        if val_id in patch_name:\n",
    "            validation_patches.append(patch_name)\n",
    "            break\n",
    "            \n",
    "    for test_id in test_ids:\n",
    "        if test_id in patch_name:\n",
    "            test_patches.append(patch_name)\n",
    "            break\n",
    "            \n",
    "# # print([x for x in train_patches if x in validation_pathes])\n",
    "print(\"train patches: {:d}, validation patches: {:d}, test patches: {:d}\".format(\n",
    "    len(train_patches), len(validation_patches), len(test_patches)))  \n",
    "\n",
    "partition = {'train': train_patches,\n",
    "             'validation': validation_patches,\n",
    "             'test': test_patches}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes:  ['Adipose', 'CIS', 'IDC', 'ILC', 'Other', 'Stroma']\n",
      "new class names:  ['unknown', 'Adipose', 'CIS', 'IDC', 'ILC', 'Other', 'Stroma']\n"
     ]
    }
   ],
   "source": [
    "# classnames = ['Mastopatic', 'CIS', 'Necrosis', 'NormalEpithelial', 'IDC', 'Stroma', 'Lymfocyten',\n",
    "#               'Adipose', 'RedBlood', 'ILC']\n",
    "classnames = ['CIS', 'IDC', 'ILC', 'Stroma', 'Adipose', 'Other']\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "if classnames is not None:\n",
    "    le.fit(classnames)\n",
    "    print('classes: ', list(le.classes_))\n",
    "    new_class_names = ['unknown'] + list(le.classes_)\n",
    "    \n",
    "print(\"new class names: \", new_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate stats per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def patches_per_slide(slide_id, classes):\n",
    "#     patches_per_class = {}\n",
    "\n",
    "def calculate_class_counts(patches):\n",
    "    class_counts = {}\n",
    "    for patch_name in patches:\n",
    "        with h5py.File(os.path.join(patches_folder, patch_name), 'r') as f:\n",
    "            seg = f['patches_20x']['segmentation'][:]\n",
    "#             patch = f['patches_20x']['patch'][:]\n",
    "            seg_ids, seg_counts = np.unique(seg, return_counts=True)\n",
    "#            print(seg_ids, seg_counts)\n",
    "            for i in range(len(seg_ids)):\n",
    "                seg_label = new_class_names[seg_ids[i]]\n",
    "                seg_count = seg_counts[i]\n",
    "            \n",
    "                if seg_label in class_counts.keys():\n",
    "                    class_counts[seg_label] += seg_count\n",
    "                else:\n",
    "                    class_counts[seg_label] = seg_count\n",
    "    return class_counts\n",
    "\n",
    "\n",
    "def returnPerc(myDict, name):\n",
    "    myDict.pop(\"unknown\", None)\n",
    "    perc = {}\n",
    "    perc['dataset'] = name\n",
    "    sum = 0\n",
    "    for i in myDict: \n",
    "        sum += myDict[i] \n",
    "    \n",
    "    for key, value in myDict.items():\n",
    "        perc[key] = value/sum\n",
    "    \n",
    "    return perc\n",
    "\n",
    "\n",
    "def metaData(patches):\n",
    "    classmeta = pd.DataFrame()\n",
    "    i = 0 \n",
    "    for si, patch_name in enumerate(patches):\n",
    "        with h5py.File(os.path.join(patches_folder, patch_name), 'r') as f:\n",
    "            seg = f['patches_20x']['segmentation'][:]\n",
    "            seg_ids, seg_counts = np.unique(seg, return_counts=True)\n",
    "            \n",
    "            if len(seg_ids) <= 1:\n",
    "                metai = pd.DataFrame(data={'patch_name': patch_name,\n",
    "                                           'class_id': list(seg_ids),\n",
    "                                           'npixels': list(seg_counts)})\n",
    "                if si == 0:\n",
    "                    classmeta = metai\n",
    "                else:\n",
    "                    classmeta = pd.concat([classmeta, metai], ignore_index=True)\n",
    "            \n",
    "            else:\n",
    "                i += 1\n",
    "    print(i)\n",
    "    return classmeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "110\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "classmeta_train = metaData(train_patches)\n",
    "classmeta_validation = metaData(validation_patches)\n",
    "classmeta_test = metaData(test_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0\n",
      "validation:  Empty DataFrame\n",
      "Columns: [patch_name, class_id, npixels]\n",
      "Index: []\n",
      "test:  Empty DataFrame\n",
      "Columns: [patch_name, class_id, npixels]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# number of patches with class \"unknown\"\n",
    "print(\"train: \", len(classmeta_train[classmeta_train.class_id == 0]))\n",
    "print(\"validation: \", classmeta_validation[classmeta_validation.class_id == 0])\n",
    "print(\"test: \", classmeta_test[classmeta_test.class_id == 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      " class_id\n",
      "1    15164\n",
      "2     1871\n",
      "3    27192\n",
      "4     5734\n",
      "5     4747\n",
      "6    10284\n",
      "Name: class_id, dtype: int64\n",
      "validation: \n",
      " class_id\n",
      "1    24540\n",
      "2      241\n",
      "3    15780\n",
      "4    10346\n",
      "5     1862\n",
      "6     2189\n",
      "Name: class_id, dtype: int64\n",
      "test: \n",
      " class_id\n",
      "3    2003\n",
      "5     503\n",
      "6     275\n",
      "Name: class_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('train: \\n', classmeta_train.groupby(['class_id'])['class_id'].count())\n",
    "print('validation: \\n', classmeta_validation.groupby(['class_id'])['class_id'].count())\n",
    "print('test: \\n', classmeta_test.groupby(['class_id'])['class_id'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_id class_label  counts\n",
      "0         1     Adipose   15164\n",
      "1         2         CIS    1871\n",
      "2         3         IDC   27192\n",
      "3         4         ILC    5734\n",
      "4         5       Other    4747\n",
      "5         6      Stroma   10284\n"
     ]
    }
   ],
   "source": [
    "train_stats = classmeta_train.groupby(['class_id'])['class_id'].count()\n",
    "df_train_stats = train_stats.to_frame(name='counts')\n",
    "df_train_stats['class_label'] =  list(le.classes_)\n",
    "df_train_stats.reset_index(inplace=True)\n",
    "print(df_train_stats[['class_id', 'class_label', 'counts']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class_id class_label  counts\n",
      "0         1     Adipose   24540\n",
      "1         2         CIS     241\n",
      "2         3         IDC   15780\n",
      "3         4         ILC   10346\n",
      "4         5       Other    1862\n",
      "5         6      Stroma    2189\n"
     ]
    }
   ],
   "source": [
    "validation_stats = classmeta_validation.groupby(['class_id'])['class_id'].count()\n",
    "df_validation_stats = validation_stats.to_frame(name='counts')\n",
    "df_validation_stats['class_label'] =  list(le.classes_)\n",
    "df_validation_stats.reset_index(inplace=True)\n",
    "print(df_validation_stats[['class_id', 'class_label', 'counts']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (6) does not match length of index (3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-55bf3d0c7794>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtest_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassmeta_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_test_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_stats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'counts'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf_test_stats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class_label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf_test_stats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test_stats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'class_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'class_label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'counts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\clam\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3038\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3039\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3040\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\clam\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3114\u001b[0m         \"\"\"\n\u001b[0;32m   3115\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3116\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3117\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\clam\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3762\u001b[0m             \u001b[1;31m# turn me into an ndarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3763\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3764\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3765\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\clam\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    746\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m         raise ValueError(\n\u001b[1;32m--> 748\u001b[1;33m             \u001b[1;34m\"Length of values \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    749\u001b[0m             \u001b[1;34mf\"({len(data)}) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[1;34m\"does not match length of index \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (6) does not match length of index (3)"
     ]
    }
   ],
   "source": [
    "test_stats = classmeta_test.groupby(['class_id'])['class_id'].count()\n",
    "df_test_stats = test_stats.to_frame(name='counts')\n",
    "df_test_stats['class_label'] =  list(le.classes_)\n",
    "df_test_stats.reset_index(inplace=True)\n",
    "print(df_test_stats[['class_id', 'class_label', 'counts']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classmeta_train.to_csv(os.path.join(patches_folder, \"classmeta_train.csv\"), index=False)\n",
    "classmeta_validation.to_csv(os.path.join(patches_folder, \"classmeta_validation.csv\"), index=False)\n",
    "classmeta_test.to_csv(os.path.join(patches_folder, \"classmeta_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classmeta_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one row at random for each distinct value in column class_id. use random_state for reproducibility:\n",
    "test = classmeta_train.groupby([\"class_id\"]).sample(n=2, random_state=1, replace=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = list(test.patch_name.values)\n",
    "\n",
    "np.random.shuffle(indexes)\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(test[test['class_id'] == 7]['patch_name'].duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnames = ['Mastopatic', 'CIS', 'Necrosis', 'NormalEpithelial', 'IDC', 'Stroma', 'Lymfocyten',\n",
    "              'Adipose', 'RedBlood', 'ILC']\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "if classnames is not None:\n",
    "    le.fit(classnames)\n",
    "    print('classes: ', list(le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classmeta_train.groupby(['class_id']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim': (256, 256),\n",
    "          'batch_size': 10, \n",
    "          'n_classes': 11,\n",
    "          'n_channels': 3,\n",
    "          'shuffle': True,\n",
    "          'data_folder': r\"D:\\annotated_slides\\separate_patches_and_labels_exp_1\"}\n",
    "\n",
    "# without augmentation\n",
    "training_generator = DataGenerator(classmeta_train, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []     # store all the generated data batches\n",
    "labels = []   # store all the generated label batches\n",
    "max_iter = 4  # maximum number of iterations, in each iteration one batch is generated; the proper value depends on batch size and size of whole data\n",
    "i = 0\n",
    "for d, l, w in training_generator:\n",
    "    data.append(d)\n",
    "    labels.append(l)\n",
    "    i += 1\n",
    "    if i == max_iter:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    \n",
    "    def __init__(self, metaData, data_folder, batch_size=10, dim=(256, 256),\n",
    "                 n_channels=3, n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.metaData = metaData\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.data_folder = data_folder\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.indexes) / self.batch_size))\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        list_IDs_temp = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "#         list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, Y, sample_weights  = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, Y, sample_weights\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        classmeta_temp = self.metaData.groupby([\"class_id\"]).sample(n=1000, replace=True)  # random_state=1\n",
    "        self.indexes = list(classmeta_temp.patch_name.values)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        Y = np.empty((self.batch_size, *self.dim))\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # print(i, ID)\n",
    "            \n",
    "            # Store sample\n",
    "            with h5py.File(os.path.join(self.data_folder, ID), 'r') as f:\n",
    "                patch = f['patches_20x']['patch'][:]\n",
    "                seg = f['patches_20x']['segmentation'][:]\n",
    "                patch = patch.astype('float') / 255\n",
    "                # seg = seg.astype('uint')\n",
    "#                 print(np.unique(seg, return_counts=True))\n",
    "                # print(patch.shape, seg.shape)\n",
    "\n",
    "                X[i, ] = patch  # f['patches_20x']['patch'][:]\n",
    "                Y[i, ] = seg  # f['patches_20x']['segmentation'][:]\n",
    "\n",
    "\n",
    "\n",
    "#         # sample weights: approach 1: 0 for unknown class and 1 for the rest\n",
    "#         sample_weights = np.ones_like(Y)\n",
    "#         sample_weights = sample_weights.astype(dtype='float32')\n",
    "#         sample_weights[np.where(Y == 0)] = 0.0\n",
    "        \n",
    "#         print(np.unique(Y, return_counts=True))\n",
    "        Y = keras.utils.to_categorical(Y, num_classes=self.n_classes)\n",
    "\n",
    "#         sample_weights = np.reshape(\n",
    "#             sample_weights, (sample_weights.shape[0],\n",
    "#                              sample_weights.shape[1] * sample_weights.shape[2]))\n",
    "\n",
    "#         # reshape to be able to use sample_weights\n",
    "#         Y = np.reshape(Y, (Y.shape[0], Y.shape[1] * Y.shape[2], -1))\n",
    "\n",
    "        return X, Y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class counts and percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts_train = calculate_class_counts(train_patches)\n",
    "class_counts_validation = calculate_class_counts(validation_patches)\n",
    "class_counts_test = calculate_class_counts(test_patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Other': 311794728, 'CIS': 125504998, 'Adipose': 994529697, 'unknown': 814688, 'IDC': 1785327230, 'Stroma': 675530731, 'ILC': 375906184}\n",
      "class_perc_train:  {'dataset': 'train', 'Other': 0.0730439014708275, 'CIS': 0.02940195546862615, 'Adipose': 0.2329876764224183, 'IDC': 0.4182471817846304, 'Stroma': 0.15825604387922837, 'ILC': 0.0880632409742693}\n",
      "\n",
      "class_perc_validation:  {'dataset': 'validation', 'Stroma': 0.039861952253206316, 'Adipose': 0.446087875232049, 'IDC': 0.28746308315669944, 'CIS': 0.0045949958175125856, 'Other': 0.03402506787637249, 'ILC': 0.18796702566416013}\n",
      "\n",
      "class_perc_test:  {'dataset': 'test', 'Stroma': 0.09881466159470999, 'IDC': 0.7200875133487574, 'Other': 0.1810978250565326}\n"
     ]
    }
   ],
   "source": [
    "print(class_counts_train)\n",
    "class_perc_train = returnPerc(class_counts_train, \"train\")\n",
    "print(\"class_perc_train: \", class_perc_train)\n",
    "print(\"\")\n",
    "class_perc_validation = returnPerc(class_counts_validation, \"validation\")\n",
    "print(\"class_perc_validation: \", class_perc_validation)\n",
    "print(\"\")\n",
    "class_perc_test = returnPerc(class_counts_test, \"test\")\n",
    "print(\"class_perc_test: \", class_perc_test)\n",
    "\n",
    "# df_class_perc_train = pd.dat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Other</th>\n",
       "      <th>CIS</th>\n",
       "      <th>Adipose</th>\n",
       "      <th>IDC</th>\n",
       "      <th>Stroma</th>\n",
       "      <th>ILC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.073044</td>\n",
       "      <td>0.029402</td>\n",
       "      <td>0.232988</td>\n",
       "      <td>0.418247</td>\n",
       "      <td>0.158256</td>\n",
       "      <td>0.088063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation</th>\n",
       "      <td>0.034025</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.446088</td>\n",
       "      <td>0.287463</td>\n",
       "      <td>0.039862</td>\n",
       "      <td>0.187967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.181098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.720088</td>\n",
       "      <td>0.098815</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Other       CIS   Adipose       IDC    Stroma       ILC\n",
       "dataset                                                               \n",
       "train       0.073044  0.029402  0.232988  0.418247  0.158256  0.088063\n",
       "validation  0.034025  0.004595  0.446088  0.287463  0.039862  0.187967\n",
       "test        0.181098       NaN       NaN  0.720088  0.098815       NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_class_perc_train = pd.DataFrame.from_dict([class_perc_train])\n",
    "df_class_perc_validation = pd.DataFrame.from_dict([class_perc_validation])\n",
    "df_class_perc_test = pd.DataFrame.from_dict([class_perc_test])\n",
    "df_class_perc = pd.concat([df_class_perc_train, df_class_perc_validation, df_class_perc_test])\n",
    "df_class_perc = df_class_perc.set_index(\"dataset\")\n",
    "df_class_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlfiles = glob.glob(os.path.join(annotation_folder,'*.xml'))\n",
    "print(\"number of annotated slides: \", len(xmlfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = classnames.insert(0, 'slide_id')\n",
    "df_annotations = pd.DataFrame(columns=column_names)\n",
    "for xmlfile in xmlfiles:\n",
    "    anno_dict = {}\n",
    "    xmlbase = os.path.splitext(os.path.basename(xmlfile))[0]\n",
    "    tree = et.parse(xmlfile)\n",
    "    root = tree.getroot()\n",
    "    annotations = root.iter('Annotation')\n",
    "    label = []\n",
    "    for elem in annotations:\n",
    "        # Loop over annotations\n",
    "        label.append(elem.get('PartOfGroup'))\n",
    "    \n",
    "    count_dict = dict(Counter(label).items())\n",
    "    \n",
    "    for name in classnames:\n",
    "        if name in count_dict.keys():\n",
    "            anno_dict[name] = count_dict[name]\n",
    "        else:\n",
    "            anno_dict[name] = 0\n",
    "            \n",
    "    anno_dict['slide_id'] = xmlbase\n",
    "    \n",
    "    df_annotations = df_annotations.append(anno_dict, ignore_index=True)\n",
    "\n",
    "# move 'slide_id column to front'\n",
    "cols = list(df_annotations)\n",
    "cols.insert(0, cols.pop(cols.index('slide_id')))\n",
    "df_annotations = df_annotations.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
